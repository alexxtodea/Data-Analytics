{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me Too: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "mt = pd.read_csv('MeTooHate.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check our dataset:\n",
    "***\n",
    "\n",
    "Because of issues with the size of the MeToo dataset I chose to clean the dataset in another project folder. The full dataset will not be added in this project you will have to download and add it yourself. \n",
    "\n",
    "This notebook is the cleaning part where i remove all useless columns and NaN values. Then I undersample the dataset because of the imbalance i category.\n",
    "\n",
    "This is then put into another CSV file that will be added into this project. I do this here to already have the dataset as small as possible for when preprocessing the text\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Thats a big dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping values\n",
    "***\n",
    "There are a lot of columns here that in my opninion are not needed for my goal. My goal is to predict if a comment is hate or non-hate. So, I decided to remove all other columns for this project except the text and category columns.\n",
    "\n",
    "Next to that i also removed all NaN values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove useless columns\n",
    "mt = mt.drop(['status_id','location', 'created_at',\n",
    "        'followers_count', 'friends_count', 'statuses_count',  'retweet_count', 'favorite_count'\n",
    "       ], axis=1)\n",
    "\n",
    "mt = mt.dropna()\n",
    "mt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This is not the first time I started cleaning this dataset. I ran into some problems last time becuase of the size of the dataset. My laptop could not handle preprocessing the text which is done in part 2 of this assignment. So I decided to change my approach. I'm now first cleaning this dataset with the goal of minimalizing the size.\n",
    "\n",
    "- first by looking at the imbalanced values in the category column. This is one thing that will help me make this dataset smaller. Next step is going to be to undersample the dataset and equalize the minority and majority class.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = mt['category'].value_counts()\n",
    "\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "#show amount of 0,1\n",
    "for i, count in enumerate(value_counts.values):\n",
    "    plt.text(i, count + 0.5, str(count), ha='center')\n",
    "\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.title('hate or no hate comments')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "After vizualizing I undersample the majority class by first declaring which is the majority and which is the minority. Then i resample the majorityclass with the amount in the minority class. Then I add the Undersampled Majority and Minority to the new dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = mt[mt['category'] == 0]\n",
    "minority_class = mt[mt['category'] == 1]\n",
    "\n",
    "undersampled_majority = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([undersampled_majority, minority_class])\n",
    "\n",
    "balanced_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Checking the size of the Majority and Minority Class\n",
    "\n",
    "Are they the same?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_majority.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the datset to minimlize it more\n",
    "***\n",
    "I was not satisfied with the size of the data set. It was still too big. So, I decided to take one fourth of the dataset and sample that. Then create a new .CSV file to use in the second part of the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = balanced_df.sample(frac=0.25, random_state=42)\n",
    "\n",
    "sampled_df.shape\n",
    "\n",
    "sampled_df.to_csv('balanced_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
